#!/usr/bin/env python
# coding: utf-8

import subprocess
import sys

MODEL_ROOT = "/opt/lm_1b"
sys.path.append(MODEL_ROOT)
import data_utils


BOS_TOKEN = "<S>"
EOS_TOKEN = "</S>"
UNK_TOKEN = "<UNK>"
MAX_WORD_LEN = 50

vocabulary = []
with open(MODEL_ROOT + "/vocab-2016-09-10.txt", "r") as vocab_f:
  for line in vocab_f:
    vocabulary.append(line.strip())
vocabulary = set(vocabulary)

tokenized = subprocess.check_output(["tokenize_inner", sys.argv[1]]).decode("utf-8").strip()
for line in tokenized.split("\n"):
  tokens = [BOS_TOKEN] + \
      [token if token in vocabulary else UNK_TOKEN for token in line.split(" ")] + \
      [EOS_TOKEN]
  print(" ".join(tokens))
